{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MSE 215: Computational Materials Science, Spring 2019\n",
    "## Department of Materials Science and Engineering | University of California, Berkeley\n",
    "\n",
    "### Instructor: Dr. Matthew Sherburne, Graduate Student Instructor: John Dagdelen\n",
    "#### Project designed by Prof. Daryl C. Chrzan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Project 4: Monte Carlo (Due May 3, 11:59PM)\n",
    "\n",
    "## Introduction\n",
    "Welcome to your Monte Carlo Laboratory Assignment. In this laboratory\n",
    "assignment, you will apply the Metropolis algorithm to compute the\n",
    "properties of a 2D Ising models.\n",
    "\n",
    "In lecture, I tried to convince you that if one considers a system\n",
    "in contact with a heat reservoir, that the probability of finding the\n",
    "system in a {\\em particular} \n",
    "microstate with energy $E$ is given by the canonical\n",
    "distribution:\n",
    "\\begin{equation}\n",
    "P(E) = \\frac{\\exp({-\\beta E})}{Z}\n",
    "\\label{prob}\n",
    "\\end{equation}\n",
    "with $\\beta = 1/k_B T$, $k_B$ defined to be Boltzmann's constant,\n",
    "and $T$ the temperature.\n",
    "The constant of proportionality, $Z$, is \n",
    "known as the partition function, and is givenby\n",
    "\n",
    "\\begin{equation}\n",
    "Z = \\sum_{\\mbox{all microstates}} \\exp({-\\beta E}) \\,\\, .\n",
    "\\label{part}\n",
    "\\end{equation}\n",
    "\n",
    "Armed with  expression (\\ref{prob}),\n",
    "one can calculate the expected result for measurement of\n",
    "any experimental parameter, provided one can perform the sum.\n",
    "\n",
    "In general, one \\emph{cannot} perform the sum.  There are simply\n",
    "too many microstates to consider.  At first blush, it appears that\n",
    "we are sunk - if we can't calculate the sum contained in\n",
    "Eq. (\\ref{part}), \n",
    "how can we hope to evaluate expectation values for the total energy,\n",
    "for example?\n",
    "\n",
    "Well, it turns out that we can get high quality estimates for the various\n",
    "parameters because of the simple fact that $P(E)$, the probability to\n",
    "find our experimental system in a microstate with energy $E$ is a sharply peaked\n",
    "function of the energy.  (Note that there is more than one accessible\n",
    "microstate with energy $E$.) Hence it is not\n",
    "necessary to evaluate the contributions from every microstate.  Rather,\n",
    "we only to need to consider the contributions from the \"important\"\n",
    "microstates.\n",
    "\n",
    "The question then arises, if we don't have to look at all of the microstates,\n",
    "how do we choose microstates such that the \"important\" states occur more\n",
    "frequently, as suggested by Eq. (\\ref{prob})?  \n",
    "A quick analysis provides the answer.\n",
    "\n",
    "Let's label all of the microstates with the subscript $n$.  Then, let \n",
    "$P_n(t)$ be the probability that the system is in the $n$th microstate\n",
    "at time $t$.  We define $\\Gamma_{n,n'}$ to the rate at which microstate\n",
    "$n$ becomes microstate $n'$.  Then one can write a set of equations,\n",
    "the so-called master equations, for the time evolution of the probabilities:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d P_n(t)}{dt} = \\sum_{n'} \\left \\{ \n",
    "\\Gamma_{n',n} P_{n'}(t) - \\Gamma_{n,n'}P_{n}(t) \\right \\} \\,\\,.\n",
    "\\label{master}\n",
    "\\end{equation}\n",
    "\n",
    "This equation just expresses the fact that the system can only transform\n",
    "between microstates - the probability is hence\n",
    "conserved.  Now at equilibrium, the $P_n(t)$ should all be\n",
    "constant.  (Otherwise, the system is not at equilibrium.)  The left hand \n",
    "side of Eq. (\\ref{master}) must be identically zero and \n",
    "the transition rates can (and must) be taken to\n",
    "satisfy:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\Gamma_{n',n}}{\\Gamma_{n,n'}} = \\frac{P_n(t)}{P_{n'}(t)} \\,\\,.\n",
    "\\label{det_balance}\n",
    "\\end{equation}\n",
    "\n",
    "*(Newman and Burkema discuss this point in their book {\\em Monte Carlo Methods in Statistical Physics} (Clarendon Press, Oxford, 2001), ISBN: 0 19 851797 1.)*\n",
    "  \n",
    "If we define the energy of the $n$th microstate to be $E_n$, then it is simple\n",
    "to show, through the use of Eqs. (\\ref{prob}) and (\\ref{part}) that\n",
    "the right hand side of Eq. (\\ref{det_balance}) is given simply by\n",
    "$\\exp(-\\beta \\Delta E)$, with $\\Delta E = E_{n'}-E_n$.  Hence at\n",
    "equilibrium, the transition rates satisfy:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Gamma_{n',n}  =\\Gamma_{n,n'} \\exp\\left (-\\beta [E_{n}-E_{n'}] \\right ) \\,\\,.\n",
    "\\label{rates}\n",
    "\\end{equation}\n",
    "\n",
    "The diagram in Fig. 1 should help to clarify the physics a bit.\n",
    "From the figure, it is clear that the rates behave in the manner you\n",
    "would expect.\n",
    "\n",
    "<img src=\"static/fig1.png\" width=\"500\"/>\n",
    "*Figure 1: A schematic diagram of the energy as a function of some unnamed reaction coordinate.  Eq. (\\ref{rates}) states that, at equilibrium, the transition rate from state A to state B is slower than the transition rate from B to A by a factor of $\\exp\\left (-\\beta \\Delta E \\right ) $.* \n",
    "\n",
    "The crux of the matter is thus the following.  At equilibrium, we know \n",
    "the relative rates at which one state transitions into another.  It can\n",
    "be shown that if we establish transition\n",
    "rates that obey Eq. (\\ref{det_balance}),\n",
    "then our system will eventually reach the true equilibrium state.\n",
    "While in the equilibrium state, the system will explore each of its\n",
    "microstates  according to the probability distribution of Eq. (\\ref{prob}).\n",
    "\\emph{That is, if we choose rates obeying Eq. (\\ref{det_balance}),\n",
    "the microstates of the ensemble will {\\bf automatically} be \n",
    "visited in proportion to their weighting in\n",
    "the probability distribution of Eq. (\\ref{prob}). This choice of rates\n",
    "assures the importance sampling we seek!} \n",
    "\n",
    "Armed with this knowledge, we can begin to explore the larger scale properties\n",
    "of an atomic scale model by merely calculating the average of\n",
    "the desired observable over the microstates generated by the\n",
    "transition rates in Eq. (\\ref{rates}) . The approach which suggests itself is the\n",
    "following.\n",
    "\n",
    "1. Develop a description of the energetics for the system in question.\n",
    "For a magnetic system, one might develop an expression for the total energy\n",
    "much like that we discussed in class.\n",
    "\n",
    "2. Develop an algorithm for transitions between microstates.  Make sure\n",
    "that the algorithm obeys detailed balance.\n",
    "\n",
    "3. Evolve the system according to the dynamics you have developed for some\n",
    "fixed \"time\".\n",
    "\n",
    "4. Calculate the value\n",
    "of the desired experimental observable and record it. \n",
    " \n",
    "5. Return to step 3 as often as \"necessary.\"\n",
    "\n",
    "6. Once you have sufficient data, calculate the average of the recorded values. This is the approximation to the ensemble average you seek.\n",
    "\n",
    "\n",
    "A comment is in order.  The dynamics we assume are just that,\n",
    "**assumed**.  The time steps do not necessarily correspond to \n",
    "steps in \"real\" time. Referring to Fig. 1, for example, we note that the\n",
    "relative transition rates between states A and B is determined by $\\Delta E$\n",
    "(and the value of $\\beta$),\n",
    "whereas the absolute rates will necessarily reflect the energy barrier\n",
    "shown in the figure.\n",
    "Second, by choosing dynamics that obey Eq. (\\ref{det_balance}) we are\n",
    "assured of approaching the equilibrium state.  However, it is not obvious that\n",
    "a \"real-world\" system obeys Eq. (\\ref{det_balance}) under all circumstances.\n",
    "In principle, the transition rates could depend upon the configuration. \n",
    "Hence the evolution from a nonequilibrium state\n",
    "towards equilibrium may not be modelled well based on the assumed dynamics.\n",
    "One is really only certain to be able to address equilibrium properties.\n",
    "So as is true with all simulation methodologies, \\emph{caveat emptor}.\n",
    "\n",
    "### Simple model for a binary alloy\n",
    "\n",
    "Suppose that we wish to study the properties of a simple 2D alloy.\n",
    "For simplicity, we will restrict the atoms to reside on the sites of a\n",
    "simple square lattice.  We consider a system that has two types of\n",
    "atoms: A and B.  Finally, we compute the energy in the system by\n",
    "summing over nearest neighbor bonds.  For example, a bond between two\n",
    "A atoms has and energy $\\varepsilon_{AA}$.  Similarly, a bond between\n",
    "two B atoms has the energy $\\varepsilon_{BB}$.  Finally, a bond\n",
    "between an A atom and a B atom has the energy $\\varepsilon_{AB}$.\n",
    "\n",
    "We define a variable $\\sigma_{i,j}=\\pm 1$ at each site of the lattice,\n",
    "$i,j$.  The site occupation variables are defined by:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "n_A(i,j)&=&\\frac{1}{2}\\left ( 1+\\sigma_{i,j} \\right ) \\\\\n",
    "n_B(i,j)&=&\\frac{1}{2}\\left ( 1-\\sigma_{i,j} \\right ) , \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $n_A(i,j)$ and $n_B(i,j)$ are the number of A and B atoms,\n",
    "respectively, at site $i,j$.\n",
    "\n",
    "Using the definitions above, the total energy of the alloy can be\n",
    "written in terms of the $\\sigma_{i,j}$'s (to within an overall constant\n",
    "that is physically unimportant):\n",
    "\n",
    "\\begin{equation}\n",
    "E_{total}=-J\\sum_{i=0}^{L-1}\\sum_{j=0}^{L-1} \\left (\\sigma_{i,j}\n",
    "  \\sigma_{i,j+1}+\\sigma_{i,j} \n",
    "  \\sigma_{i+1,j} \\right )-H\\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} \\sigma_{i,j}, \\label{2DIsing}\n",
    "\\end{equation}\n",
    "\n",
    "with $J=\\left ( 2 \\varepsilon_{AB}-\\varepsilon_{AA}-\\varepsilon_{BB}\n",
    "\\right )/4$, and $H=\\left (\\varepsilon_{BB}-\\varepsilon_{AA} \\right)$. We will assume periodic boundary conditions so that the index $i=L$\n",
    "corresponds to $i=0$ and the index $i=-1$ corresponds to the index\n",
    "$i=L-1$, etc.  Note that if $J>0$, or equivalently, if\n",
    "$\\varepsilon_{AA}+\\varepsilon_{BB}<2 \\varepsilon_{AB}$, the model\n",
    "corresponds to a ferromagnet or phase separating alloy.  If $J<0$,\n",
    "then the model yields an antiferromagnet or an ordering alloy.\n",
    "Similarly, if $H>0$, the model favors atoms of type A.\n",
    "\n",
    "The model described in Eq. (\\ref{2DIsing}) is known as the 2D Ising\n",
    "Model.   It holds a unique place in the history of statistical\n",
    "mechanics, as it is, perhaps, the simplest exactly solved statistical mechanics\n",
    "model that displays a finite temperature phase transition. In what\n",
    "follows, we will interpret the model as that for a ferromagnet.\n",
    "In doing so, we will avoid some subtleties associated with\n",
    "interpreting the model as an alloy. \n",
    "\n",
    "### Metropolis algorithm for the 2D Ising model\n",
    "\n",
    "I think that the best way to understand  Monte Carlo techniques\n",
    "is to implement one, and that is what you will do. You can code the\n",
    "model in your own favorite programming language, but the code must be\n",
    "yours - do not simply find one on the web.   I suggest using either\n",
    "Matlab or Mathematica, but you are free to use whatever language you\n",
    "want, as long as there are sufficiently sophisticated math libraries\n",
    "that include a good random number generator, and, perhaps, functions\n",
    "for computing correlations. A word of caution, though.  Monte Carlo simulations\n",
    "can require substantial computer resources to produce reliable\n",
    "results.  Keep this in mind when choosing your computational\n",
    "platform.\n",
    "\n",
    "So, we will calculate the properties of the 2-D Ising model using\n",
    "Monte Carlo simulation methods.  We have already taken the first step in\n",
    "this endeavor - we've developed an expression for the energy of the\n",
    "microstates.  The next step is the development of an algorithm which\n",
    "insures that Eq. (\\ref{rates}) is obeyed at all times.\n",
    "\n",
    "The algorithm we are about to discuss was developed by Metropolis and\n",
    "coworkers in 1953, and hence is known\n",
    "as the Metropolis algorithm (see original paper posted on the bcourses\n",
    "site).  The steps taken are the following.\n",
    "\n",
    "1. Establish an initial configuration of moments. One may give the moments\n",
    "random (with respect to up or down) orientations, \n",
    "or one may start with some predetermined configuration.\n",
    "\n",
    "2. Select a moment within the lattice at random.  Compute the\n",
    "  change in energy associated with flipping the spin, $\\Delta E \\left\n",
    "    ( \\sigma_{i,j}\\rightarrow -\\sigma_{i,j} \\right )$.\n",
    "\n",
    "3. If $\\Delta E \\leq 0$, \\emph{i.e.} the energy decreases upon\n",
    "  flipping the moment, ''accept'' the change.  That is, assume that\n",
    "  the next microstate in the Monte Carlo trajectory is the one you\n",
    "  generated by flipping the spin.  If $\\Delta E > 0$, ''accept'' the\n",
    "  new configuration with a probability given by $\\exp (-\\beta \\Delta\n",
    "  E)$.  This can be accomplished by generating a random number $\\eta$,\n",
    "  and comparing it with $\\exp (-\\beta \\Delta E)$.  If $\\eta <\\exp\n",
    "  (-\\beta \\Delta E)$ accept the new configuration as the next in the\n",
    "  trajectory, otherwise, keep the original configuration as the next\n",
    "  in the trajectory.\n",
    "\n",
    "4. Compute quantities of interest for this step in the trajectory.\n",
    "  You will want to compute the magnetization and the total energy at\n",
    "  each step.\n",
    "\n",
    "5. Repeat from step 2 as often as ''necessary.'' \n",
    "\n",
    "6. Analyze the results of the simulation.\n",
    "\n",
    "\\end{enumerate}\n",
    "\n",
    "This procedure sounds simple enough, and in fact, it is. But there are\n",
    "significant details that need to be addressed.  For example, though\n",
    "the dynamics we employ assuredly take the system to equilibrium, the\n",
    "initial configuration of spins is typically far from equilibrium.  It\n",
    "is necessary, therefore, to run a number of steps before assuming that\n",
    "the microstates correspond to those associated with the equilibrium\n",
    "distribution.  Unfortunatley, estimating the number of steps required\n",
    "to equilibrate is non-trivial (though we discuss this below).  \n",
    "\n",
    "### ''random'' numbers\n",
    "Random numbers lie at the heart of our Monte Carlo enterprise.  There\n",
    "is one obvious difficulty, however.  Computers are deterministic -\n",
    "given a specific code, the same input always produces the same output\n",
    "(in this case we include as inputs things that the code might read\n",
    "from a clock etc.)  So numbers generated by a computer are {\\em never}\n",
    "truly random.\n",
    "\n",
    "However, given the importance of generating apparently random numbers,\n",
    "the coding and mathematics communities have put some effort into\n",
    "generating numbers that for our intents and purposes appear random.\n",
    "The details of the most advanced methods are beyond the scope of this\n",
    "course.  However, we can describe the simple random number generators\n",
    "that were used in times passed, and note that modern instances retain\n",
    "some similarities.\n",
    "\n",
    "Basically, the generation of random numbers is accomplished through \n",
    "a discrete mapping. According to the authors of\n",
    "*Numerical Recipes*,\\footnote{William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery, \\emph{Numerical Recipes: The Art of Scientific Computing}, Third Edition, 2007 Cambridge University Press, ISBN-10: 0521880688.}  a typical mapping appears like this:\n",
    "\n",
    "\\begin{equation}\n",
    " I_{j+1} =\\mbox{mod}(a I_j,m)\n",
    "\\end{equation}\n",
    "\n",
    "where the function mod is used to return the remainder of the first argument\n",
    "divided by the second.  The result of this calculation is an integer between\n",
    "0 and $m-1$, inclusive.  Note that these numbers are necessarily\n",
    "periodic - at most,\n",
    "one can generate one of m possible answers.  It turns out that the quality of\n",
    "the random number generator is determined by the values chosen for $a$ and for\n",
    "$m$.  According to Park and Miller\\footnote{S. K. \n",
    "Park and K. W. Miller, \\emph{Communications of the ACM} \\underline{31}, \n",
    "1192 (1988), as quoted in \n",
    "\\emph{Numerical Recipes}.} the optimal choice for 32 bit\n",
    "machines is $a=16807$ and $m = 2^{31}-1 = 2147483647$.\n",
    "\n",
    "Modern random number generators are much more sophisticated, but most\n",
    "still rely on a seed and are periodic (at least in principle ... the\n",
    "periods are so large that one cannot typically verify this directly).\n",
    "\n",
    "In our work then, we will assume that you will use the best random\n",
    "number generator available to you, and that this random number\n",
    "generator will be sufficiently sophisticated so as to produce a string\n",
    "of reliably random numbers for our simulations. This is certainly true of the\n",
    "random number generators in Matlab and Mathematica.\n",
    "\n",
    "### finite size effects\n",
    "\n",
    "Throughout our discussion of statistical mechanics we have assumed\n",
    "that our experimental system has an enormous number of degrees of\n",
    "freedom.  In conducting Monte Carlo simulations, we are constrained by\n",
    "computational resources to consider systems with only a relatively\n",
    "small number of degrees of freedom.  It is natural then, to consider\n",
    "what effect, if any, the finite size of our simulation might have on\n",
    "our predictions.\n",
    "\n",
    "In this respect, the physics of the phase transition in the 2D Ising\n",
    "model is of interest.  First we note that at high temperatures, the\n",
    "magnetization of the model should approach zero.  At these\n",
    "temperatures, thermal fluctuations become significant, and the spins,\n",
    "in general, become quite disordered.  Another way to see this is to\n",
    "examine the algorithm that we discussed above.  As\n",
    "$T\\rightarrow\\infty$, $\\beta\\rightarrow 0$.  The result is that the probability\n",
    "to accept any spin flip goes to $1$, and the thermodynamic average of\n",
    "any spin approaches zero as well.\n",
    "\n",
    "As the temperature is reduced from $\\infty$, spin flips that increase\n",
    "the energy are accepted with a reduced probability.  Thus spins will\n",
    "begin to align with their neighboring spins (for a ferromagnetic\n",
    "interaction) and domains in which the spins are aligned will become\n",
    "apparent. At higher temperatures, these domains will be small, and\n",
    "will fluctuate quite rapidly in time.  However, as the temperature is\n",
    "decreased, the domain size will, in fact, increase.  Eventually, at a\n",
    "critical value of the temperature known as $T_c$, the domain size will\n",
    "diverge.  The temperature demarcates the temperature boundary of the\n",
    "phase transition to the ferromagnetic phase.  Below $T_c$, the system\n",
    "will display ferromagnetism.  Above $T_c$, the system is a paramagnet.\n",
    "\n",
    "The domain size can be explored mathematically.  Define the spin-spin\n",
    "correlation, $g_{\\sigma}(\\Delta_i,\\Delta_j)$, to be:\n",
    "\n",
    "\\begin{equation}\n",
    "g_{\\sigma}(\\Delta_i,\\Delta_j)=\\frac{\\langle\n",
    "  \\sigma_{i+\\Delta_i,j+\\Delta_j}\\sigma_{i,j} \\rangle -\\langle\n",
    "  \\sigma_{i,j} \\rangle^2 }{\\langle\n",
    "  \\sigma_{i,j}^2  \\rangle -\\langle\n",
    "  \\sigma_{i,j} \\rangle^2 }\n",
    "\\end{equation}\n",
    "\n",
    "where $\\langle A \\rangle$ denotes the thermodynamic average of the\n",
    "quantity $A$.  Briefly, this function measures the\n",
    "response of the system to perturbations. More specifically, if you\n",
    "were to perturb a system at the origin, say by holding it fixed in the\n",
    "up position, this function measures how strongly this perturbation is\n",
    "felt at the point $\\Delta_i,\\Delta_j$.  Typically, we expect that\n",
    "$g_{\\sigma}(\\Delta_i,\\Delta_j) \\sim \\exp \\left (\n",
    "  -\\sqrt{\\Delta_i^2+\\Delta_j^2}/\\xi \\right )$, with $\\xi$ the\n",
    "correlation length.  This correlation length serves as a good measure\n",
    "of the domain size the model.\n",
    "\n",
    "The problem with finite sized systems, then, becomes readily\n",
    "apparent. If the correlation length of the system diverges in an\n",
    "infinite system, then at some point, the same correlation length\n",
    "becomes equal to the size of our finite sized system.  This implies\n",
    "that our predictions must be impacted by our choice of size for the\n",
    "system. The periodic boundary conditions limit the physical\n",
    "fluctuations within the model to wavelengths shorter than the size of\n",
    "the system - this limit does not exist in the macroscopic object.\n",
    "\n",
    "The truncation of fluctuations due to finite size affects our\n",
    "predictions.  First and foremost, the divergence of the correlation\n",
    "length that demarcates the temperature $T_c$ is no longer possible -\n",
    "our finite size model does not, formally, display a phase transition.\n",
    "This means that one of the quantities we may be very interested in,\n",
    "$T_c$, is not directly measurable. (Finite size scaling, however,\n",
    "enables us to estimate these effects directly.)\n",
    "\n",
    "A second consequence of the finite size of our system is that when we\n",
    "time average the magnetization for very long times, we will always\n",
    "find the value 0, irrespective of temperature.  The reason for this is\n",
    "a bit subtle, but important to understand.  In the absence of a\n",
    "magnetic field, the Ising model spins have no preference for point up\n",
    "or down.  For temperatures below $T_c$, the system spontaneously\n",
    "chooses a preferred direction up or down (this is a so-called\n",
    "spontaneous symmetry breaking).  For temperatures below $T_c$, the\n",
    "energy barrier encountered in flipping the direction of the average\n",
    "moment within a macroscopic system is infinite, and consequently, the\n",
    "magnetization of the model remains in the chosen orientation\n",
    "indefinitely.  However, in our finite sized models, this is not the\n",
    "case. The barrier for a complete spin flip is finite, and at\n",
    "temperatures below, but near $T_c$, the complete spin flip becomes\n",
    "possible.\n",
    "\n",
    "As a work around, we will ultimately compute $\\langle | m|\\rangle$ instead of\n",
    "$\\langle m \\rangle$, with $m$ the magnetization per site:\n",
    "\n",
    "\\begin{equation}\n",
    "m=\\frac{1}{L^2} \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} \\sigma_{i,j}.\n",
    "\\end{equation}\n",
    "\n",
    "Clearly, there are differences between  $\\langle | m|\\rangle$ and\n",
    "$\\langle m \\rangle$.  For example, $\\langle | m|\\rangle$ will never go\n",
    "to zero for $T>T_c$, whereas $\\langle m \\rangle$ should.  Nevertheless,\n",
    "using \n",
    "$\\langle | m|\\rangle$ will prove more convenient for what follows.\n",
    "\n",
    "### estimating uncertainty\n",
    "\n",
    "Monte Carlo simulations are statistical in nature.  This means that\n",
    "unlike the computation of the total energy from an interatomic\n",
    "potential, for example, Monte Carlo simulations are always numerical\n",
    "expeeriments.   Consequently, we must always estimate the errors in\n",
    "our prediction.\n",
    "\n",
    "A classical analysis of uncertainty leads the following estimator for the\n",
    "uncertainty in a quantity $m$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta m = \\sqrt{\\frac {\\frac{1}{n}\\sum_{j=1}^{n} \\left ( m_i - \\langle\n",
    "      m \\rangle \\right )^2}{n-1}} \\label{variance}\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle m \\rangle = \\frac{1}{n}\\sum_{j=1}^{n} m_i ,\n",
    "\\end{equation}\n",
    "\n",
    "and the final prediction for $m$ equal to\n",
    "\n",
    "\\begin{equation}\n",
    "m_{predicted}=\\langle m \\rangle \\pm \\Delta m.\n",
    "\\end{equation}\n",
    "\n",
    "It seems that we should be able to use Eq. (\\ref{variance}) in our analysis.\n",
    "There is a problem, however.  Equation (\\ref{variance}) assumes that\n",
    "the $n$ trials are statistically independent. If we\n",
    "use each microstate in our Monte Carlo trajectory, are the points over\n",
    "which we are averaging truly statistically independent?\n",
    "\n",
    "To answer this question, we must consider the time correlation\n",
    "function of the absolute value of the magnetization per site, $g_m(\\tau)$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "g_m(\\tau) & = & \\frac{\\langle \\left ( |m(t+\\tau)|  -\\langle |m(t)|\n",
    "    \\rangle \\right )  \\left ( |m(t)|  -\\langle |m(t)|\n",
    "    \\rangle \\right ) \\rangle}{\\langle |m(t)|^2 \\rangle - \\langle\n",
    "  |m(t)| \\rangle ^2 } \\\\\n",
    "& = &\\frac{\\langle |m(t+\\tau)| |m(t)|\\rangle -\\langle |m(t)|\n",
    "    \\rangle^2} {\\langle |m(t)|^2 \\rangle - \\langle\n",
    "  |m(t)| \\rangle ^2 }. \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "Here, I have used the variable $t$ to represent the sequence of the\n",
    "microstates generated by our Monte Carlo sampling scheme. It is\n",
    "important to remember that, since the dynamics are fictitious, these\n",
    "do not correspond to real times.  Nevertheless, it is convenient to\n",
    "refer to these as times, and we will do so.\n",
    "\n",
    "Like the spin correlation function we discussed above, $g_m(\\tau)$\n",
    "measures the correlation between two quantities.  In this case, we are\n",
    "measuring the time scale over which a perturbation in magnetization\n",
    "persists.  Typically, $g_m(\\tau) \\sim \\exp \\left ( -\\tau/\\tau_o \\right\n",
    ")$, with $\\tau_o$ known as the correlation time.  If one perturbs the\n",
    "magnetization at time $t=0$, the perturbation will be, essentially,\n",
    "washed out within a time $2 \\tau_o$.  (We note that at a time $\\tau_o$, the\n",
    "sample is still correlated \\- $g_m(\\tau_o) \\sim 1/e$.) \n",
    "\n",
    "The implication, then, is that microstates within the Monte Carlo\n",
    "trajectory are truly independent if they are separated by a time\n",
    "difference of $2 \\tau$. Thus the number of independent trials we have\n",
    "in our simulation, $n_{trials}$, is simply:\n",
    "\n",
    "\\begin{equation}\n",
    "n_{trials}=\\frac{n_{steps}}{2 \\tau_o}\n",
    "\\end{equation}\n",
    "\n",
    "where $n_{steps}$ is the number of trajectories within your Monte\n",
    "Carlo simulation.  With this identification, \n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta m = \\sqrt{\\frac {\\frac{1}{n_{steps}}\\sum_{j=1}^{n_steps} \n",
    "\\left ( m_i - \\langle m \\rangle \\right )^2}{n_{trials}-1}}.\\label{finalvariance}\n",
    "\\end{equation}\n",
    "\n",
    "Assessing the accuracy of your predictions, then, requires that you\n",
    "are able to measure the correlation time of your data.  This\n",
    "measurement can be nontrivial.  For example, the brute force method\n",
    "that you might envision is very expensive computationally, and, quite\n",
    "frankly, not workable.\n",
    "\n",
    "Fortunately, there are sophisticated schemes for computing the\n",
    "necessary correlations that rely heavily on an algorithm known as an\n",
    "FFT, a fast-Fourier-transform.   If you are using Matlab or\n",
    "Mathematica for your coding, there are built in correlation function\n",
    "calculating tools (xcorr in Matlab and CorrelationFunction in\n",
    "Mathematica).  You can read about how these work (nominally) in the\n",
    "book {\\em Numerical Recipes}, and I recommend doing so.  It is very\n",
    "important when using a pre-coded routine that you understand what the\n",
    "routine computes.  (The Matlab documentation for xcorr seems a bit\n",
    "weak here...  You may have to develop simple test cases to see how the\n",
    "routine works.)\n",
    "\n",
    "With the correlation function computed, you can then assess the rate\n",
    "of decay. There are several options for finding the correlation time.\n",
    "One is fit the correlation function data to the exponential form, and\n",
    "then to use the fitted parameter for $\\tau_o$ as the correlation\n",
    "time.  This procedure, however, is a bit involved.  As Newman and\n",
    "Barkema point out in their book {\\em Monte Carlo Methods in\n",
    "  Statistical Physics}, there is an easier way.  If we integrate the\n",
    "exponential function over times from 0 to $\\infty$, the result of the\n",
    "integral is simply $\\tau_o$.  Once you have the correlation function\n",
    "computed, you can simply numerically estimate this integral by summing\n",
    "the terms between $\\tau=0$ and the value of $\\tau$ at which\n",
    "$g_m(\\tau)$ first falls below zero (or out to the largest value of\n",
    "$\\tau$ that you have, if the result does not fall to zero. (If you\n",
    "encounter this case, you probably need to execute more Monte Carlo\n",
    "steps.)\n",
    "\n",
    "Armed with this value of the correlation time, you are in the\n",
    "position to compute the uncertainty in expectation values.  You can do\n",
    "this using Eq. (\\ref{finalvariance}) though this is not, strictly,\n",
    "optimal.  The problem is that in estimating the heat capacity, for\n",
    "example, the uncertainties in $\\langle E \\rangle$ and $\\langle E^2\n",
    "\\rangle$ are not linearly independent, so combining the errors to\n",
    "estimate the error in the heat capacity itself is tricky.  Instead,\n",
    "Newman and Berkema suggest a better approach known as the bootstrap\n",
    "method.\n",
    "\n",
    "The procedure for estimating uncertainty within the bootstrap method\n",
    "is the following.  First, using the correlation time, estimate how\n",
    "many independent trials you have within your data set, $n_{trials}$.\n",
    "Then, draw $n_{trials}$ microstates from the trajectory (with\n",
    "replacement, so that the same data set can be drawn repeatedly), and\n",
    "use these to compute the quantity of interest, call this $q_i$.  Repeat this process\n",
    "''many'' times, $i=1$ through $i=n_{max}$.  Then, the uncertainty in the\n",
    "reported value of $\\langle q \\rangle$ is given simply by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta q = \\sqrt{\\langle q^2 \\rangle -\\langle q \\rangle^2},\n",
    "\\end{equation}\n",
    "\n",
    "with $\\langle q \\rangle$ and $\\langle q^2 \\rangle$ computed using the\n",
    "$n_{max}$ trials.\n",
    "You can then report your prediction as $\\langle q \\rangle \\pm \\Delta\n",
    "q$,\n",
    "where $\\langle q \\rangle$ is computed from the complete data set\n",
    "(excepting the steps over which the system is approaching equilibrium).\n",
    "\n",
    "## your assignment\n",
    "\n",
    "Given this preliminary information, we are now in the position to\n",
    "consider the assignment. Simply put, you are to build a Monte Carlo\n",
    "simulation of the 2D Ising model with $H=0$.  Using this model, you\n",
    "are to compute the expectation values of the absolute value of the\n",
    "magnetization per spin for systems of $L \\times L$ spins, with\n",
    "$L=4, 8, 16$, and $32$ as a function of temperature.\n",
    "\n",
    "It is most convenient to work in dimensionless units.  We will, therefore, compute all energies in units of $J$.  All temperatures will be measured in units of $J/k_B$.  Making the substitutions: $E \\rightarrow \\tilde{E} J$, $H \\rightarrow \\tilde{H} J$, with quantities with tildes dimensionless, the dimensionless energy expression becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{E}_{total}=-\\sum_{i=0}^{L-1}\\sum_{j=0}^{L-1} \\left (\\sigma_{i,j}\n",
    "  \\sigma_{i,j+1}+\\sigma_{i,j} \n",
    "  \\sigma_{i+1,j} \\right )-\\tilde{H}\\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} \\sigma_{i,j}.\n",
    "\\end{equation}\n",
    "\n",
    "One can also show that the partition function can be written in dimensionless form as:\n",
    "\n",
    "\\begin{equation}\n",
    "Z=\\sum_{\\{\\sigma_{ij} \\} } \\exp \\left [   \\frac{1}{\\tilde{T}} \\sum_{i=0}^{L-1}\\sum_{j=0}^{L-1} \\left (\\sigma_{i,j}\n",
    "  \\sigma_{i,j+1}+\\sigma_{i,j} \n",
    "  \\sigma_{i+1,j} \\right ) +\\frac{\\tilde{H}}{\\tilde{T}} \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} \\sigma_{i,j} \\right ] ,\n",
    "\\end{equation}\n",
    "\n",
    "with the sum extending over all spin configurations.  You can then compute the heat capacity per site using the formulae developed in class.\n",
    "\n",
    "In addition, you should compute the magnetic susceptibility per site, $\\chi$,\n",
    "at each temperature:\n",
    "\n",
    "\\begin{equation}\n",
    "\\chi = \\frac{ \\left ( \\langle M^2 \\rangle - \\langle M \\rangle^2\n",
    "   \\right )  }{L^2 \\tilde{T}}.\n",
    "\\end{equation}\n",
    "\n",
    "Here $M$ is the total magnetization of the system.\n",
    "You should also compute the heat capacity per site for the model as\n",
    "well. You should estimate the uncertainty in all of your predictions.\n",
    "\n",
    "Plot the magnetizations, the susceptibilities and heat capacities,\n",
    "with the appropriate error bars as a function of temperature, and\n",
    "compare them to the exact values for the infinite system presented\n",
    "below.  Discuss the dependence of your results on system size.\n",
    "\n",
    "Here are the exact solution predictions for the 2-D Ising\n",
    "model with $H=0$.  (The solution is due to Onsager.) The magnetization per site\n",
    "is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle m \\rangle = \\left \\{ 1 - \\left [\\sinh \\left ( 2 \\beta J \\right\n",
    "    )\\right ]^{-4}\n",
    "\\right \\}^\\frac{1}{8}.\n",
    "\\end{equation}\n",
    "\n",
    "The critical temperature is defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{2 J}{k_B T_c}= \\ln \\left ( 1+\\sqrt{2} \\right ).\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the heat capacity per spin, $C$, is given by\n",
    "\n",
    "\\begin{equation}\n",
    "C= \\frac{2 k_B}{\\pi}\\left ( \\frac{2 J}{k_B T_c} \\right )^2 \\left [ -\\ln \\left\n",
    "    ( 1-\\frac{T}{T_c} \\right )+ \\ln \\left ( \\frac{k_B T_c}{2 J} \\right\n",
    "  ) - \\left (1+\\frac{\\pi}{4} \\right ) \\right ].\n",
    "\\end{equation}\n",
    "\n",
    "Note that the heat capacity diverges at the critical point logarithmically.  This divergence is not seen in a finite sized system.  Note also, that I do not have an analytical solution for the magnetic susceptibility, so you do not have to compare this to the exact result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mse215] *",
   "language": "python",
   "name": "conda-env-mse215-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
